{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and prep\n",
    "\n",
    "**objectives**\n",
    "* load the songs from the zip file\n",
    "* perform transformations to prepare the data for two-tower training\n",
    "\n",
    "**steps**\n",
    "1. Create a bq dataset\n",
    "2. Load the million playlist data to Big Query\n",
    "3. Create pipelines to download audio and artist features for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFIX = ndr-v1\n"
     ]
    }
   ],
   "source": [
    "# naming convention for all cloud resources\n",
    "VERSION        = \"v1\"                  # TODO\n",
    "PREFIX         = f'ndr-{VERSION}'      # TODO\n",
    "\n",
    "print(f\"PREFIX = {PREFIX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROJECT_ID               = \"myproject32549\"\n",
      "PROJECT_NUM              = \"683169793466\"\n",
      "LOCATION                 = \"us-central1\"\n",
      "\n",
      "REGION                   = \"us-central1\"\n",
      "BQ_LOCATION              = \"US\"\n",
      "VPC_NETWORK_NAME         = \"ucaip-haystack-vpc-network\"\n",
      "\n",
      "VERTEX_SA                = \"683169793466-compute@developer.gserviceaccount.com\"\n",
      "\n",
      "PREFIX                   = \"ndr-v1\"\n",
      "VERSION                  = \"v1\"\n",
      "\n",
      "APP                      = \"sp\"\n",
      "MODEL_TYPE               = \"2tower\"\n",
      "FRAMEWORK                = \"tfrs\"\n",
      "DATA_VERSION             = \"v1\"\n",
      "TRACK_HISTORY            = \"5\"\n",
      "\n",
      "BUCKET_NAME              = \"ndr-v1-myproject32549-bucket\"\n",
      "BUCKET_URI               = \"gs://ndr-v1-myproject32549-bucket\"\n",
      "SOURCE_BUCKET            = \"spotify-million-playlist-dataset\"\n",
      "\n",
      "DATA_GCS_PREFIX          = \"data\"\n",
      "DATA_PATH                = \"gs://ndr-v1-myproject32549-bucket/data\"\n",
      "VOCAB_SUBDIR             = \"vocabs\"\n",
      "VOCAB_FILENAME           = \"vocab_dict.pkl\"\n",
      "\n",
      "CANDIDATE_PREFIX         = \"candidates\"\n",
      "TRAIN_DIR_PREFIX         = \"train\"\n",
      "VALID_DIR_PREFIX         = \"valid\"\n",
      "\n",
      "VPC_NETWORK_FULL         = \"projects/683169793466/global/networks/ucaip-haystack-vpc-network\"\n",
      "\n",
      "BQ_DATASET               = \"spotify_e2e_test\"\n",
      "BQ_TABLE_TRAIN           = \"train_flatten_last_5\"\n",
      "BQ_TABLE_VALID           = \"train_flatten_valid_last_5\"\n",
      "BQ_TABLE_CANDIDATES      = \"candidates\"\n",
      "\n",
      "REPO_SRC                 = \"src\"\n",
      "PIPELINES_SUB_DIR        = \"feature_pipes\"\n",
      "\n",
      "REPOSITORY               = \"ndr-v1-spotify\"\n",
      "IMAGE_NAME               = \"train-v1\"\n",
      "REMOTE_IMAGE_NAME        = \"us-central1-docker.pkg.dev/myproject32549/ndr-v1-spotify/train-v1\"\n",
      "DOCKERNAME               = \"tfrs\"\n",
      "\n",
      "SERVING_IMAGE_URI_CPU    = \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-11:latest\"\n",
      "SERVING_IMAGE_URI_GPU    = \"us-docker.pkg.dev/vertex-ai/prediction/tf2-gpu.2-11:latest\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# staging GCS\n",
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "# GCS bucket and paths\n",
    "BUCKET_NAME              = f'{PREFIX}-{PROJECT_ID}-bucket'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "config = !gsutil cat {BUCKET_URI}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set your variables for your project, region, and dataset name\n",
    "SOURCE_BUCKET = 'spotify-million-playlist-dataset'\n",
    "PROJECT_ID = 'myproject32549'\n",
    "REGION = 'us-central1'\n",
    "BQ_DATASET = 'spotify_e2e_test2'\n",
    "\n",
    "import time\n",
    "from google.cloud import bigquery\n",
    "\n",
    "bigquery_client = bigquery.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create BigQuery dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a bigquery dataset (one time operation)\n",
    "# Construct a full Dataset object to send to the API.\n",
    "dataset = bigquery.Dataset(f\"`{PROJECT_ID}.{BQ_DATASET}`\")\n",
    "dataset.location = REGION\n",
    "\n",
    "# Send the dataset to the API for creation, with an explicit timeout.\n",
    "# Raises google.api_core.exceptions.Conflict if the Dataset already\n",
    "# exists within the project.\n",
    "\n",
    "# dataset = bigquery_client.create_dataset(BQ_DATASET, timeout=30)  # Make an API request.\n",
    "\n",
    "# print(\"Created dataset {}.{}\".format(bigquery_client.project, dataset.dataset_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data from GCS\n",
    "\n",
    "(also `curl` from source, see readme)\n",
    "\n",
    "* This step can take up to 30 minutes\n",
    "* Iteration occurs over 1000 json files if you are using the full dataset\n",
    "* This should give you a `playlists` bq data set with 1,076,000 rows (playlists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gsutil cp gs://{SOURCE_BUCKET}/spotify_million_playlist_dataset.zip .\n",
    "# !unzip -n spotify_million_playlist_dataset.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spotify-million-playlist-dataset\n"
     ]
    }
   ],
   "source": [
    "print(SOURCE_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BigQuery data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import playlists\n",
    "* load JSON files to BigQuery\n",
    "* playlists are nested as one large string that needs to be parsed (use json compatible functionality for BQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1000 [00:03<11:01,  1.50it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 45\u001b[0m\n\u001b[1;32m     36\u001b[0m     append_df\u001b[38;5;241m.\u001b[39mto_gbq(\n\u001b[1;32m     37\u001b[0m         destination_table\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBQ_DATASET\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.playlists\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     38\u001b[0m         project_id\u001b[38;5;241m=\u001b[39mPROJECT_ID, \u001b[38;5;66;03m# TODO: param\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m         if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     43\u001b[0m     )\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 45\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     46\u001b[0m     append_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df, append_df])\n\u001b[1;32m     47\u001b[0m     count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[12], line 12\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(filename: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 12\u001b[0m         json_dict \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m         df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(json_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplaylists\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     14\u001b[0m         df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtracks\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtracks\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28mstr\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[1;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[1;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/codecs.py:319\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_buffer_decode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, errors, final):\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;66;03m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;66;03m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[0;32m--> 319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m    322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer_decode(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors, final)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "\n",
    "data_files = os.listdir('./data')\n",
    "\n",
    "def load_data(filename: str):\n",
    "    \n",
    "    with open(f'data/{filename}') as f:\n",
    "        json_dict = json.load(f)\n",
    "        df = pd.DataFrame(json_dict['playlists'])\n",
    "        df['tracks'] = df['tracks'].map(str)\n",
    "        #write to bq\n",
    "    return df\n",
    "        \n",
    "# make sure there is not already existing data in the playlists table\n",
    "# loops over json files - converts to pandas then upload/appends\n",
    "\n",
    "### add this if you want to limit to smaller number of playlists - this scales significantly later!\n",
    "n_playliststs_limit = None #add if you want to use in for loop: while counter <= n_playliststs_limit:\n",
    "total_files = len(data_files)\n",
    "count = 0\n",
    "batch_size = 15\n",
    "\n",
    "for filename in tqdm(data_files):\n",
    "    if count == 0 or (count-1) % batch_size == 0:\n",
    "        append_df = load_data(filename)\n",
    "        count += 1\n",
    "    \n",
    "    if count % batch_size == 0 or count == total_files:\n",
    "        df = load_data(filename) \n",
    "        append_df = pd.concat([df, append_df])\n",
    "        count += 1\n",
    "        append_df.to_gbq(\n",
    "            destination_table=f'{BQ_DATASET}.playlists', \n",
    "            project_id=PROJECT_ID, # TODO: param\n",
    "            location='US', \n",
    "            progress_bar=False, \n",
    "            reauth=True, \n",
    "            if_exists='append'\n",
    "        )\n",
    "    else:\n",
    "        df = load_data(filename) \n",
    "        append_df = pd.concat([df, append_df])\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### check table\n",
    "\n",
    "<!-- ![](img/tracks-string.png) -->\n",
    "<img\n",
    "  src=\"img/tracks-string.png\"\n",
    "  alt=\"Alt text\"\n",
    "  title=\"BQ table: playlists\"\n",
    "  style=\"display: inline-block; margin: 0 auto; max-width: 1200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested playlists\n",
    "\n",
    "* run parameterized queries to shape the data\n",
    "* This query formats the json strings to be read as Bigquery structs, to be manipulated in subsequent queries\n",
    "* Creates `playlists_nested` by parsing the string data to a struct with arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.1 ms, sys: 820 µs, total: 34.9 ms\n",
      "Wall time: 37.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table._EmptyRowIterator at 0x7fba5d93a0b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "json_extract_query = f\"\"\"create or replace table `{PROJECT_ID}.{BQ_DATASET}.playlists_nested` as (\n",
    "with json_parsed as (SELECT * except(tracks), JSON_EXTRACT_ARRAY(tracks) as json_data FROM `{PROJECT_ID}.{BQ_DATASET}.playlists` )\n",
    "\n",
    "select json_parsed.* except(json_data),\n",
    "ARRAY(SELECT AS STRUCT\n",
    "JSON_EXTRACT_SCALAR(json_data, \"$.pos\") as pos, \n",
    "JSON_EXTRACT_SCALAR(json_data, \"$.artist_name\") as artist_name,\n",
    "JSON_EXTRACT_SCALAR(json_data, \"$.track_uri\") as track_uri,\n",
    "JSON_EXTRACT_SCALAR(json_data, \"$.artist_uri\") as artist_uri,\n",
    "JSON_EXTRACT_SCALAR(json_data, \"$.track_name\") as track_name,\n",
    "JSON_EXTRACT_SCALAR(json_data, \"$.album_uri\") as album_uri,\n",
    "JSON_EXTRACT_SCALAR(json_data, \"$.duration_ms\") as duration_ms,\n",
    "JSON_EXTRACT_SCALAR(json_data, \"$.album_name\") as album_name\n",
    "from json_parsed.json_data\n",
    ") as tracks,\n",
    "from json_parsed) \"\"\"\n",
    "\n",
    "bigquery_client.query(json_extract_query).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check table\n",
    "\n",
    "<!-- ![](img/playlists-nested.png) -->\n",
    "<img\n",
    "  src=\"img/playlists-nested.png\"\n",
    "  alt=\"Alt text\"\n",
    "  title=\"BQ table: playlists_nested\"\n",
    "  style=\"display: inline-block; margin: 0 auto; max-width: 1200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Unique tracks\n",
    "\n",
    "* This table will then be used to call the Spotify API and enrich with additional data about each track and artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.7 ms, sys: 394 µs, total: 21 ms\n",
      "Wall time: 6.88 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table._EmptyRowIterator at 0x7f09d8170f70>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "unique_tracks_sql = f\"\"\"create or replace table `{PROJECT_ID}.{BQ_DATASET}.tracks_unique` as (\n",
    "SELECT distinct \n",
    "    track.track_uri,\n",
    "    track.album_uri,\n",
    "    track.artist_uri, \n",
    "FROM `{PROJECT_ID}.{BQ_DATASET}.playlists_nested`, UNNEST(tracks) as track)\n",
    "\"\"\"\n",
    "\n",
    "bigquery_client.query(unique_tracks_sql).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data loading complete**\n",
    "\n",
    "* We now have our unique id tables we will use for grabbing additional audio and artist features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVKyuWRezBGF"
   },
   "source": [
    "# Spotify API Feature Extraction Pipeline\n",
    "___________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgGWdClhze9D",
    "tags": []
   },
   "source": [
    "**references**\n",
    "* Spotify Mlllion Playlist Dataset Challenge [Homepage](https://www.aicrowd.com/challenges/spotify-million-playlist-dataset-challenge)\n",
    "* [Spotify Web API docs](https://developer.spotify.com/documentation/web-api/reference/#/)\n",
    "\n",
    "**Community Examples**\n",
    "* [Extracting song lists](https://github.com/tojhe/recsys-spotify/blob/master/processing/songlist_extraction.py)\n",
    "* [construct audio features with Spotify API](https://github.com/tojhe/recsys-spotify/blob/master/processing/audio_features_construction.py)\n",
    "* [Using Spotify API](https://towardsdatascience.com/extracting-song-data-from-the-spotify-api-using-python-b1e79388d50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spotify Developer Dashboard**\n",
    "<!-- ![](img/spotify-dev-console.png) -->\n",
    "<!-- <img src=\"https://github.com/jswortz/spotify_mpd_two_tower/blob/main/img/spotify-dev-console.png\"  width=\"600\" height=\"300\"> -->\n",
    "\n",
    "<img\n",
    "  src=\"img/spotify-dev-console.png\"\n",
    "  alt=\"Alt text\"\n",
    "  title=\"Spotify Developer Dashboard\"\n",
    "  style=\"display: inline-block; margin: 0 auto; max-width: 900px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO - required**\n",
    "* In your repo, create `spotipy_secret_creds.py`,\n",
    "* assign file to .gitignore\n",
    "* define variables:\n",
    "> * SPOTIPY_CLIENT_ID='YOUR_CLIENT_ID'\n",
    "> * SPOTIPY_CLIENT_SECRET='YOUR_CLIENT_SECRET'\n",
    "> * SPOTIFY_USERNAME='YOUR_USERNAME'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# local py file\n",
    "import spotipy_secret_creds as creds\n",
    "\n",
    "# SPOTIPY_CLIENT_ID=creds.SPOTIPY_CLIENT_ID\n",
    "# SPOTIPY_CLIENT_SECRET=creds.SPOTIPY_CLIENT_SECRET\n",
    "\n",
    "SPOTIPY_CLIENT_ID=\"c9dfb2904ba941a192265351fb847bae\"\n",
    "SPOTIPY_CLIENT_SECRET=\"b4213910730e448faae6df2d90f2674e\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**local json credentials - optional**\n",
    "* if you are concerned about visibility to api keys (credentials), please see [GCP Secret Manager](https://cloud.google.com/secret-manager)\n",
    "* Below is an example if you were to add the json file to secret manager (keys: `secret`, `id`)\n",
    "\n",
    "```python\n",
    "from google.cloud import secretmanager\n",
    "\n",
    "###Note you copy/paste this from secret manager in console\n",
    "SECRET_VERSION = 'projects/934903580331/secrets/spotify-creds1/versions/1'\n",
    "\n",
    "sm_client = secretmanager.SecretManagerServiceClient()\n",
    "\n",
    "name = sm_client.secret_path(PROJECT_ID, SECRET_ID)\n",
    "\n",
    "response = client.access_secret_version(request={\"name\": SECRET_VERSION})   \n",
    "\n",
    "payload = json.loads(response.payload.data.decode(\"UTF-8\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get spotify credentials\n",
    "# This file has id and secret stored as attributes\n",
    "\n",
    "###########################################\n",
    "### CAUTION THIS APPROACH WILL HAVE THE CREDENTIALS APPEAR IN THE CONSOLE - \n",
    "### USE SECRET MANAGER APPROACH IN EACH COMPONENT AS NEEDED (PROVIDED ABOVE)\n",
    "###########################################\n",
    "\n",
    "# # json \n",
    "# creds = open('spotify-creds.json')\n",
    "# spotify_creds = json.load(creds)\n",
    "# creds.close()\n",
    "\n",
    "# SPOTIPY_CLIENT_ID=spotify_creds['id']\n",
    "# SPOTIPY_CLIENT_SECRET=spotify_creds['secret']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pip & package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spotipy in /home/jupyter/.local/lib/python3.10/site-packages (2.23.0)\n",
      "Requirement already satisfied: redis>=3.5.3 in /home/jupyter/.local/lib/python3.10/site-packages (from spotipy) (5.0.3)\n",
      "Requirement already satisfied: requests>=2.25.0 in /opt/conda/lib/python3.10/site-packages (from spotipy) (2.31.0)\n",
      "Requirement already satisfied: six>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from spotipy) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.10/site-packages (from spotipy) (1.26.18)\n",
      "Requirement already satisfied: async-timeout>=4.0.3 in /opt/conda/lib/python3.10/site-packages (from redis>=3.5.3->spotipy) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->spotipy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->spotipy) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->spotipy) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spotipy google-cloud-storage google-cloud-aiplatform gcsfs --user -q\n",
    "!pip install --user kfp google-cloud-pipeline-components --upgrade -q\n",
    "!pip install --user -q google-cloud-secret-manager\n",
    "!pip install spotipy --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "irhbKrY3fcG8",
    "outputId": "2008a7ca-303d-4943-e56b-a57040c9abd7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 2.7.0\n",
      "google_cloud_pipeline_components version: 2.13.1\n",
      "aiplatform SDK version: 1.48.0\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\"\n",
    "! python3 -c \"import google.cloud.aiplatform; print('aiplatform SDK version: {}'.format(google.cloud.aiplatform.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "GqhJlsodR-xX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import time\n",
    "\n",
    "import gcsfs\n",
    "\n",
    "# GCP\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import storage\n",
    "\n",
    "# Pipelines\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "\n",
    "# Kubeflow SDK\n",
    "# TODO: fix these\n",
    "from kfp.v2 import dsl\n",
    "import kfp\n",
    "import kfp.v2.dsl\n",
    "from kfp.v2.google import client as pipelines_client\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCbr2tLuzTJK"
   },
   "source": [
    "### env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7wwktfnCyzWO",
    "outputId": "6afe4a17-ebc2-40dc-d23a-1f447f394046",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_TAG: pipe-v1-spotify-feature-enrich\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = 'myproject32549' #update\n",
    "LOCATION = 'us-central1' \n",
    "\n",
    "BUCKET_NAME = 'matching-engine-content-fred'\n",
    "\n",
    "VERSION = 'v1'\n",
    "PIPELINE_VERSION = f'pipe-v1' # pipeline code\n",
    "PIPELINE_TAG = f'{PIPELINE_VERSION}-spotify-feature-enrich'\n",
    "\n",
    "print(\"PIPELINE_TAG:\", PIPELINE_TAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create bucket if needed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://matching-engine-content-fred/...\n"
     ]
    }
   ],
   "source": [
    "!gsutil mb -l $LOCATION gs://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuJk9A5I9ZfF"
   },
   "source": [
    "### client & credentials\n",
    "* Setup Vertex AI client for pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GCbHe1G_gICe",
    "outputId": "c3f7d046-4997-4b59-9bcc-44df8feba2ed",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Setup clients\n",
    "vertex_ai.init(\n",
    "    project=PROJECT_ID, \n",
    "    location=LOCATION, \n",
    "    staging_bucket=BUCKET_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QbeNm6_whXMG"
   },
   "source": [
    "## Create pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘src/feature_pipes’: File exists\n"
     ]
    }
   ],
   "source": [
    "# REPO_SRC = 'src'\n",
    "# PIPELINES_SUB_DIR = 'feature_pipes'\n",
    "\n",
    "# ! rm -rf {REPO_SRC}/{PIPELINES_SUB_DIR}\n",
    "! mkdir {REPO_SRC}/{PIPELINES_SUB_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8m3D974DjTbE"
   },
   "source": [
    "### component: track audio features\n",
    "\n",
    "[Link to artist API and related features we will pull](https://developer.spotify.com/documentation/web-api/reference/#/operations/get-audio-features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/feature_pipes/call_spotify_api_audio.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_SRC}/{PIPELINES_SUB_DIR}/call_spotify_api_audio.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.10.14\",\n",
    "    packages_to_install=[\n",
    "        'fsspec', 'google-cloud-bigquery',\n",
    "        'google-cloud-storage',\n",
    "        'gcsfs',\n",
    "        'spotipy','requests','db-dtypes',\n",
    "        'numpy','pandas','pyarrow','absl-py', 'pandas-gbq==0.19.0',\n",
    "        'tqdm'\n",
    "    ]\n",
    ")\n",
    "def call_spotify_api_audio(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    client_id: str,\n",
    "    batch_size: int,\n",
    "    batches_to_store: int,\n",
    "    target_table: str,\n",
    "    client_secret: str,\n",
    "    unique_table: str,\n",
    "    sleep_param: float,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('done_message', str),\n",
    "]):\n",
    "    print(f'pip install complete')\n",
    "    import os\n",
    "    \n",
    "    import spotipy\n",
    "    from spotipy.oauth2 import SpotifyClientCredentials\n",
    "    \n",
    "    import re\n",
    "    import warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    \n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import time\n",
    "    import numpy as np\n",
    "    \n",
    "    from requests.exceptions import ReadTimeout, HTTPError, ConnectionError, RequestException\n",
    "    from absl import logging\n",
    "    \n",
    "    from google.cloud import storage\n",
    "    import gcsfs\n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    import pandas_gbq\n",
    "    from multiprocessing import Process\n",
    "    from tqdm import tqdm\n",
    "    from tqdm.contrib.logging import logging_redirect_tqdm\n",
    "    \n",
    "    from google.cloud.exceptions import NotFound\n",
    "\n",
    "    import multiprocessing\n",
    "\n",
    "    # print(f'package import complete')\n",
    "\n",
    "    logging.set_verbosity(logging.INFO)\n",
    "    logging.info(f'package import complete')\n",
    "\n",
    "    \n",
    "    bq_client = bigquery.Client(\n",
    "      project=project, location=location\n",
    "    )\n",
    "    \n",
    "    def spot_audio_features(uri, client_id, client_secret):\n",
    "\n",
    "        # Authenticate\n",
    "        client_credentials_manager = SpotifyClientCredentials(\n",
    "            client_id=client_id, \n",
    "            client_secret=client_secret\n",
    "        )\n",
    "        sp = spotipy.Spotify(\n",
    "            client_credentials_manager = client_credentials_manager, \n",
    "            requests_timeout=10, \n",
    "            retries=10\n",
    "        )\n",
    "        ############################################################################\n",
    "        # Create Track Audio Features DF\n",
    "        ############################################################################\n",
    "        \n",
    "        uri_stripped = [u.replace('spotify:track:', '') for u in uri] #fix the quotes \n",
    "        #getting track popularity\n",
    "        tracks = sp.tracks(uri_stripped)\n",
    "        #Audio features\n",
    "        time.sleep(sleep_param)\n",
    "    \n",
    "        a_feats = sp.audio_features(uri)\n",
    "        features = pd.json_normalize(a_feats)#.to_dict('list')\n",
    "        \n",
    "        features['track_pop'] = pd.json_normalize(tracks['tracks'])['popularity']\n",
    "        \n",
    "        features['track_uri'] = uri\n",
    "        return features\n",
    "\n",
    "    bq_client = bigquery.Client(\n",
    "        project=project, \n",
    "        location='US'\n",
    "    )\n",
    "    \n",
    "    #check if target table exists and if so return a list to not duplicate records\n",
    "    try:\n",
    "        bq_client.get_table(target_table)  # Make an API request.\n",
    "        logging.info(\"Table {} already exists.\".format(target_table))\n",
    "        target_table_incomplete_query = f\"select distinct track_uri from `{target_table}`\"\n",
    "        loaded_tracks_df = bq_client.query(target_table_incomplete_query).result().to_dataframe()\n",
    "        loaded_tracks = loaded_tracks_df.track_uri.to_list()\n",
    "        \n",
    "    except NotFound:\n",
    "        logging.info(\"Table {} is not found.\".format(target_table))\n",
    "    \n",
    "    query = f\"select distinct track_uri from `{unique_table}`\" \n",
    "\n",
    "    #refactor\n",
    "    schema = [{'name':'danceability', 'type': 'FLOAT', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'energy', 'type': 'FLOAT', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'key', 'type': 'FLOAT', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'loudness', 'type': 'FLOAT', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'mode', 'type': 'INTEGER', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'speechiness', 'type': 'FLOAT', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'acousticness', 'type': 'FLOAT', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'instrumentalness', 'type': 'FLOAT', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'liveness', 'type': 'FLOAT', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'valence', 'type': 'FLOAT', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'tempo', 'type': 'FLOAT', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'type', 'type': 'STRING', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'id', 'type': 'STRING', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'uri', 'type': 'STRING', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'track_href', 'type': 'STRING', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'analysis_url', 'type': 'STRING', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'duration_ms', 'type': 'INTEGER', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'time_signature', 'type': 'INTEGER', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'track_pop', 'type': 'INTEGER', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'track_uri', 'type': 'STRING', \"mode\": \"REQUIRED\"},\n",
    "    ]\n",
    "    \n",
    "    tracks = bq_client.query(query).result().to_dataframe()\n",
    "    track_list = tracks.track_uri.to_list()\n",
    "    logging.info(f'finished downloading tracks')\n",
    "    \n",
    "    \n",
    "    ### This section is used when there are tracks already loaded into BQ and you want to resume loading the data\n",
    "    try:\n",
    "        track_list = list(set(track_list) - set(loaded_tracks)) #sets the new track list to remove already loaded data in BQ\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    from tqdm import tqdm\n",
    "    def process_track_list(track_list):\n",
    "        \n",
    "        uri_list_length = len(track_list)-1 #starting count at zero\n",
    "        inner_batch_count = 0 #avoiding calling the api on 0th iteration\n",
    "        uri_batch = []\n",
    "        \n",
    "        for i, uri in enumerate(tqdm(track_list)):\n",
    "            uri_batch.append(uri)\n",
    "            if (len(uri_batch) == batch_size or uri_list_length == i) and i > 0: #grab a batch of 50 songs\n",
    "                    # logging.info(f\"appending final record for nth song at: {inner_batch_count} \\n i: {i} \\n uri_batch length: {len(uri_batch)}\")\n",
    "                    ### Try catch block for function\n",
    "                try:\n",
    "                    audio_featureDF = spot_audio_features(uri_batch, client_id, client_secret)\n",
    "                    time.sleep(sleep_param)\n",
    "                    uri_batch = []\n",
    "                except ReadTimeout:\n",
    "                    logging.info(\"'Spotify timed out... trying again...'\")\n",
    "                    audio_featureDF = spot_audio_features(uri_batch, client_id, client_secret)\n",
    "                    \n",
    "                    uri_batch = []\n",
    "                    time.sleep(sleep_param)\n",
    "                \n",
    "                except HTTPError as err: #JW ADDED\n",
    "                    logging.info(f\"HTTP Error: {err}\")\n",
    "                \n",
    "                except spotipy.exceptions.SpotifyException as spotify_error: #jw_added\n",
    "                    logging.info(f\"Spotify error: {spotify_error}\")\n",
    "                    \n",
    "                # Accumulate batches on the machine before writing to BQ\n",
    "                # if inner_batch_count <= batches_to_store or uri_list_length == i:\n",
    "                if inner_batch_count == 0:\n",
    "                    appended_data = audio_featureDF\n",
    "                    # logging.info(f\"creating new appended data at IBC: {inner_batch_count} \\n i: {i}\")\n",
    "                    inner_batch_count += 1\n",
    "                elif uri_list_length == i or inner_batch_count == batches_to_store: #send the batches to bq\n",
    "                    appended_data = pd.concat([audio_featureDF, appended_data])\n",
    "                    inner_batch_count = 0\n",
    "                    appended_data.to_gbq(\n",
    "                        destination_table=target_table, \n",
    "                        project_id=f'{project}', \n",
    "                        location='US', \n",
    "                        table_schema=schema,\n",
    "                        progress_bar=False, \n",
    "                        reauth=False, \n",
    "                        if_exists='append'\n",
    "                    )\n",
    "                    logging.info(f'{i+1} of {uri_list_length} complete!')\n",
    "                else:\n",
    "                    appended_data = pd.concat([audio_featureDF, appended_data])\n",
    "                    inner_batch_count += 1\n",
    "\n",
    "        logging.info(f'audio features appended')\n",
    "    \n",
    "    #multiprocessing portion - we will loop based on the modulus of the track_uri list\n",
    "    #chunk the list \n",
    "    \n",
    "    # Yield successive n-sized\n",
    "    # chunks from l.\n",
    "    def divide_chunks(l, n):\n",
    "        # looping till length l\n",
    "        for i in range(0, len(l), n):\n",
    "            yield l[i:i + n]\n",
    "            \n",
    "    n_cores = multiprocessing.cpu_count() \n",
    "    chunked_tracks = list(divide_chunks(track_list, int(len(track_list)/n_cores))) #produces a list of lists chunked evenly by groups of n_cores\n",
    "    \n",
    "    logging.info(\n",
    "        f\"\"\"\n",
    "        total tracks downloaded: {len(track_list)}\\n\n",
    "        length of chunked_tracks: {len(chunked_tracks)}\\n \n",
    "        and inner dims: {[len(x) for x in chunked_tracks]}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    procs = []\n",
    "    \n",
    "    def create_job(target, *args):\n",
    "        p = multiprocessing.Process(target=target, args=args)\n",
    "        p.start()\n",
    "        return p\n",
    "\n",
    "    # starting process with arguments\n",
    "    for track_chunk in chunked_tracks:\n",
    "        proc = create_job(process_track_list, track_chunk)\n",
    "        time.sleep(np.pi)\n",
    "        procs.append(proc)\n",
    "\n",
    "    # complete the processes\n",
    "    for proc in procs:\n",
    "        proc.join()\n",
    "        \n",
    "    # process_track_list(track_list) #single thread\n",
    "     \n",
    "    logging.info(f'audio features appended')\n",
    "    \n",
    "    return (\n",
    "          f'DONE',\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hppPSwkh1x2",
    "tags": []
   },
   "source": [
    "### component: artist metadata \n",
    "\n",
    "[Link to artist API and related features we will pull](https://developer.spotify.com/documentation/web-api/reference/#/operations/get-an-artist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "JYbgsLrtxPHl",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/feature_pipes/call_spotify_api_artist.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_SRC}/{PIPELINES_SUB_DIR}/call_spotify_api_artist.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "### Artist tracks api call\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.10.14\",\n",
    "    packages_to_install=[\n",
    "        'fsspec',' google-cloud-bigquery',\n",
    "        'google-cloud-storage',\n",
    "        'gcsfs', 'tqdm',\n",
    "        'spotipy','requests','db-dtypes',\n",
    "        'numpy','pandas','pyarrow','absl-py', 'pandas-gbq==0.19.0',\n",
    "        'google-cloud-secret-manager'\n",
    "    ]\n",
    ")\n",
    "def call_spotify_api_artist(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    unique_table: str,\n",
    "    batch_size: int,\n",
    "    batches_to_store: int,\n",
    "    client_id: str,\n",
    "    client_secret: str,\n",
    "    sleep_param: float,\n",
    "    target_table: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('done_message', str),\n",
    "]):\n",
    "    print(f'pip install complete')\n",
    "    \n",
    "    import os\n",
    "    \n",
    "    import spotipy\n",
    "    from spotipy.oauth2 import SpotifyClientCredentials\n",
    "    \n",
    "    import re\n",
    "    import warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    \n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import time\n",
    "    import numpy as np\n",
    "    \n",
    "    from google.cloud import storage\n",
    "    import gcsfs\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    from requests.exceptions import ReadTimeout, HTTPError, ConnectionError, RequestException\n",
    "    from absl import logging\n",
    "\n",
    "    import pandas_gbq\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "    from google.cloud.exceptions import NotFound\n",
    "    \n",
    "    from multiprocessing import Process\n",
    "    import multiprocessing\n",
    "\n",
    "    logging.set_verbosity(logging.INFO)\n",
    "    logging.info(f'package import complete')\n",
    "\n",
    "    storage_client = storage.Client(\n",
    "        project=project\n",
    "    )\n",
    "    \n",
    "    logging.info(f'spotipy auth complete')\n",
    "    \n",
    "    def spot_artist_features(uri, client_id, client_secret):\n",
    "\n",
    "        # Authenticate\n",
    "        client_credentials_manager = SpotifyClientCredentials(\n",
    "            client_id=client_id, \n",
    "            client_secret=client_secret\n",
    "        )\n",
    "        sp = spotipy.Spotify(\n",
    "            client_credentials_manager = client_credentials_manager, \n",
    "            requests_timeout=2, \n",
    "            retries=1 )\n",
    "\n",
    "        ############################################################################\n",
    "        # Create Track Audio Features DF\n",
    "        ############################################################################ \n",
    "\n",
    "        # uri = [u.replace('spotify:artist:', '') for u in uri] #fix the quotes \n",
    "\n",
    "        artists = sp.artists(uri)\n",
    "        features = pd.json_normalize(artists['artists'])\n",
    "        smaller_features = features[['genres', 'popularity', 'name', 'followers.total']]\n",
    "        smaller_features.columns = ['genres_list',  'artist_pop', 'name',  'followers']\n",
    "        smaller_features['artist_uri'] = uri\n",
    "        smaller_features['genres'] = smaller_features['genres_list'].map(lambda x: f\"{x}\")\n",
    "        return smaller_features[['genres', 'artist_pop', 'artist_uri', 'followers']]\n",
    "        \n",
    "\n",
    "    bq_client = bigquery.Client(\n",
    "      project=project, location='US'\n",
    "    )\n",
    "\n",
    "    #check if target table exists and if so return a list to not duplicate records\n",
    "    try:\n",
    "        bq_client.get_table(target_table)  # Make an API request.\n",
    "        logging.info(\"Table {} already exists.\".format(target_table))\n",
    "        target_table_incomplete_query = f\"select distinct artist_uri from `{target_table}`\"\n",
    "        loaded_tracks_df = bq_client.query(target_table_incomplete_query).result().to_dataframe()\n",
    "        loaded_tracks = loaded_tracks_df.artist_uri.to_list()\n",
    "        \n",
    "    except NotFound:\n",
    "        logging.info(\"Table {} is not found.\".format(target_table))\n",
    "        \n",
    "        \n",
    "    query = f\"select distinct artist_uri from `{unique_table}`\"\n",
    "    \n",
    "\n",
    "    schema = [\n",
    "        {'name': 'artist_pop', 'type': 'INTEGER'},\n",
    "        {'name':'genres', 'type': 'STRING'},\n",
    "        {'name':'followers', 'type': 'INTEGER'},\n",
    "        {'name':'artist_uri', 'type': 'STRING'}\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    tracks = bq_client.query(query).result().to_dataframe()\n",
    "    track_list = tracks.artist_uri.to_list()\n",
    "    logging.info(f'finished downloading tracks')\n",
    "    \n",
    "    from tqdm import tqdm\n",
    "    def process_track_list(track_list):\n",
    "        uri_list_length = len(track_list)-1 #starting count at zero\n",
    "        inner_batch_count = 0 #avoiding calling the api on 0th iteration\n",
    "        uri_batch = []\n",
    "        for i, uri in enumerate(tqdm(track_list)):\n",
    "            uri_batch.append(uri)\n",
    "            if (len(uri_batch) == batch_size or uri_list_length == i) and i > 0: #grab a batch of 50 songs\n",
    "                ### Try catch block for function\n",
    "                try:\n",
    "                    audio_featureDF = spot_artist_features(uri_batch, client_id, client_secret)\n",
    "                    time.sleep(sleep_param)\n",
    "                    uri_batch = []\n",
    "                \n",
    "                except ReadTimeout:\n",
    "                    logging.info(\"'Spotify timed out... trying again...'\")\n",
    "                    audio_featureDF = spot_artist_features(uri_batch, client_id, client_secret)\n",
    "                    uri_batch = []\n",
    "                    time.sleep(sleep_param)\n",
    "                \n",
    "                except HTTPError as err: #JW ADDED\n",
    "                    logging.info(f\"HTTP Error: {err}\")\n",
    "                \n",
    "                except spotipy.exceptions.SpotifyException as spotify_error: #jw_added\n",
    "                    logging.info(f\"Spotify error: {spotify_error}\")\n",
    "                \n",
    "                # Accumulate batches on the machine before writing to BQ\n",
    "                # if inner_batch_count <= batches_to_store or uri_list_length == i:\n",
    "                \n",
    "                if inner_batch_count == 0:\n",
    "                    appended_data = audio_featureDF\n",
    "                    # logging.info(f\"creating new appended data at IBC: {inner_batch_count} \\n i: {i}\")\n",
    "                    inner_batch_count += 1\n",
    "                \n",
    "                elif uri_list_length == i or inner_batch_count == batches_to_store: #send the batches to bq\n",
    "                    appended_data = pd.concat([audio_featureDF, appended_data])\n",
    "                    inner_batch_count = 0\n",
    "                    appended_data.to_gbq(\n",
    "                        destination_table=target_table, \n",
    "                        project_id=f'{project}', \n",
    "                        location='US', \n",
    "                        table_schema=schema,\n",
    "                        progress_bar=False, \n",
    "                        reauth=False, \n",
    "                        if_exists='append'\n",
    "                    )\n",
    "                    logging.info(f'{i+1} of {uri_list_length} complete!')\n",
    "                \n",
    "                else:\n",
    "                    appended_data = pd.concat([audio_featureDF, appended_data])\n",
    "                    inner_batch_count += 1\n",
    "\n",
    "        logging.info(f'audio features appended')\n",
    "    \n",
    "    #multiprocessing portion - we will loop based on the modulus of the track_uri list\n",
    "    #chunk the list \n",
    "    \n",
    "    # Yield successive n-sized\n",
    "    # chunks from l.\n",
    "    def divide_chunks(l, n):\n",
    "        # looping till length l\n",
    "        for i in range(0, len(l), n):\n",
    "            yield l[i:i + n]\n",
    "            \n",
    "    n_cores = multiprocessing.cpu_count() \n",
    "    chunked_tracks = list(divide_chunks(track_list, int(len(track_list)/n_cores))) # produces a list of lists chunked evenly by groups of n_cores\n",
    "    \n",
    "    logging.info(\n",
    "        f\"\"\"total tracks downloaded: {len(track_list)}\\n\n",
    "        length of chunked_tracks: {len(chunked_tracks)}\\n \n",
    "        and inner dims: {[len(x) for x in chunked_tracks]}\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    procs = []\n",
    "    def create_job(target, *args):\n",
    "        p = multiprocessing.Process(target=target, args=args)\n",
    "        p.start()\n",
    "        return p\n",
    "\n",
    "    # starting process with arguments\n",
    "    for track_chunk in chunked_tracks:\n",
    "        proc = create_job(process_track_list, track_chunk)\n",
    "        time.sleep(np.pi)\n",
    "        procs.append(proc)\n",
    "\n",
    "    # complete the processes\n",
    "    for proc in procs:\n",
    "        proc.join()\n",
    "        \n",
    "    # process_track_list(track_list)\n",
    "    logging.info(f'artist features appended')\n",
    "    \n",
    "    return (\n",
    "          f'DONE',\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ma5qzpGkuYhD"
   },
   "source": [
    "## Build pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO_SRC: src\n",
      "PIPELINES_SUB_DIR: feature_pipes\n"
     ]
    }
   ],
   "source": [
    "print(f'REPO_SRC: {REPO_SRC}')\n",
    "print(f'PIPELINES_SUB_DIR: {PIPELINES_SUB_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "Buwtyt7rugt4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.feature_pipes import call_spotify_api_audio, call_spotify_api_artist\n",
    "\n",
    "@kfp.v2.dsl.pipeline(\n",
    "  name=f'spotify-feature-enrichment-{PIPELINE_TAG}'.replace('_', '-')\n",
    ")\n",
    "def pipeline(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    unique_table: str,\n",
    "    target_table_audio: str,\n",
    "    target_table_artist: str,\n",
    "    target_table_popularity: str,\n",
    "    batch_size: int,\n",
    "    batches_to_store: int,\n",
    "    sleep_param: float,\n",
    "    spotify_id: str = SPOTIPY_CLIENT_ID, # = spotify_creds['id'],\n",
    "    spotify_secret: str = SPOTIPY_CLIENT_SECRET, # = spotify_creds['secret'],\n",
    "):\n",
    "    \n",
    "    call_spotify_api_artist_op = (\n",
    "        call_spotify_api_artist.call_spotify_api_artist(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            client_id=spotify_id,\n",
    "            client_secret=spotify_secret,\n",
    "            batch_size=batch_size,\n",
    "            sleep_param=sleep_param,\n",
    "            unique_table=unique_table,\n",
    "            target_table=target_table_artist,\n",
    "            batches_to_store=batches_to_store,\n",
    "        )\n",
    "        .set_display_name(\"Get Artist Features From Spotify API\")\n",
    "    )\n",
    "\n",
    "    call_spotify_api_audio_op = (\n",
    "        call_spotify_api_audio.call_spotify_api_audio(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            client_id=spotify_id,\n",
    "            client_secret=spotify_secret,\n",
    "            batch_size=batch_size,\n",
    "            sleep_param=sleep_param,\n",
    "            unique_table=unique_table,\n",
    "            target_table=target_table_audio,\n",
    "            batches_to_store=batches_to_store,\n",
    "        )\n",
    "        .set_display_name(\"Get Track Audio Features From Spotify API\")\n",
    "        .after(call_spotify_api_artist_op)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile pipeline \n",
    "* compiles to json\n",
    "* store in GCS bucket for tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RoBsfT1IyIXd",
    "outputId": "50a65461-1c7c-44c0-9362-5524aaac21c4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PIPELINE_JSON_SPEC_LOCAL = \"custom_track_meta_pipeline_spec.json\"\n",
    "\n",
    "! rm -f $PIPELINE_JSON_SPEC_LOCAL\n",
    "\n",
    "kfp.v2.compiler.Compiler().compile(\n",
    "  pipeline_func=pipeline, \n",
    "  package_path=PIPELINE_JSON_SPEC_LOCAL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINES_FILEPATH: gs://matching-engine-content-fred/v1/pipeline_root/custom_track_meta_pipeline_spec.json\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_ROOT_PATH = f'gs://{BUCKET_NAME}/{VERSION}/pipeline_root'\n",
    "\n",
    "PIPELINES_FILEPATH = f'{PIPELINE_ROOT_PATH}/{PIPELINE_JSON_SPEC_LOCAL}'\n",
    "\n",
    "print(\"PIPELINES_FILEPATH:\", PIPELINES_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'custom_track_meta_pipeline_spec.json'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PIPELINE_JSON_SPEC_LOCAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'matching-engine-content-fred'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://matching-engine-content-fred/v1/pipeline_root/custom_track_meta_pipeline_spec.json'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PIPELINES_FILEPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://custom_track_meta_pipeline_spec.json [Content-Type=application/json]...\n",
      "- [1 files][ 24.9 KiB/ 24.9 KiB]                                                \n",
      "Operation completed over 1 objects/24.9 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp $PIPELINE_JSON_SPEC_LOCAL $PIPELINES_FILEPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit pipeline\n",
    "\n",
    "* Set pipeline parameters dict\n",
    "* create pipeline job\n",
    "* submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set pipeline params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "evUgOHllykr5",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'project': 'myproject32549',\n",
       " 'location': 'us-central1',\n",
       " 'unique_table': 'myproject32549.spotify_e2e_test2.tracks_unique',\n",
       " 'target_table_audio': 'myproject32549.spotify_e2e_test2.audio_features',\n",
       " 'target_table_artist': 'myproject32549.spotify_e2e_test2.artist_features',\n",
       " 'target_table_popularity': 'myproject32549.spotify_e2e_test2.track_popularity',\n",
       " 'batch_size': 50,\n",
       " 'batches_to_store': 400,\n",
       " 'sleep_param': 0.5}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BUCKET_NAME = 'matching-engine-content'\n",
    "\n",
    "ideal_batch_size = 20_000\n",
    "bts = ideal_batch_size / 50\n",
    "\n",
    "PIPELINE_PARAMETERS = dict(\n",
    "    project = PROJECT_ID,\n",
    "    location = 'us-central1',\n",
    "    unique_table = f'{PROJECT_ID}.{BQ_DATASET}.tracks_unique', \n",
    "    target_table_audio = f'{PROJECT_ID}.{BQ_DATASET}.audio_features',\n",
    "    target_table_artist = f'{PROJECT_ID}.{BQ_DATASET}.artist_features',\n",
    "    target_table_popularity = f'{PROJECT_ID}.{BQ_DATASET}.track_popularity',\n",
    "    batch_size = 50,\n",
    "    batches_to_store = int(bts),\n",
    "    sleep_param = .5,\n",
    "    # spotify_id = SPOTIPY_CLIENT_ID,\n",
    "    # spotify_secret = SPOTIPY_CLIENT_SECRET\n",
    ")\n",
    "\n",
    "PIPELINE_PARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create pipeline job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name = f'spotify-feature-enrichment-{PIPELINE_TAG}'.replace('_', '-'),\n",
    "    template_path = PIPELINE_JSON_SPEC_LOCAL,\n",
    "    pipeline_root = f'gs://{BUCKET_NAME}/{VERSION}',\n",
    "    parameter_values = PIPELINE_PARAMETERS,\n",
    "    project = PROJECT_ID,\n",
    "    location = LOCATION,\n",
    "    enable_caching=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:spotipy auth complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip install complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Table myproject32549.spotify_e2e_test2.artist_features already exists.\n",
      "INFO:absl:finished downloading tracks\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "range() arg 3 must not be zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 215\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    210\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDONE\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    211\u001b[0m       )\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# client_id=\"f4c3b0103b144195888f586f767cb748\"\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# client_secret=\"d36e9279fa8e4f12b98ef72a2e1f6a72\"\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m \u001b[43mcall_spotify_api_artist\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmyproject32549\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmyproject32549\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43munique_table\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmyproject32549.spotify_e2e_test2.tracks_unique\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatches_to_store\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf4c3b0103b144195888f586f767cb748\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_secret\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43md36e9279fa8e4f12b98ef72a2e1f6a72\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_param\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_table\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmyproject32549.spotify_e2e_test2.artist_features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 193\u001b[0m, in \u001b[0;36mcall_spotify_api_artist\u001b[0;34m(project, location, unique_table, batch_size, batches_to_store, client_id, client_secret, sleep_param, target_table)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m l[i:i \u001b[38;5;241m+\u001b[39m n]\n\u001b[1;32m    192\u001b[0m n_cores \u001b[38;5;241m=\u001b[39m multiprocessing\u001b[38;5;241m.\u001b[39mcpu_count() \n\u001b[0;32m--> 193\u001b[0m chunked_tracks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdivide_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrack_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrack_list\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mn_cores\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# produces a list of lists chunked evenly by groups of n_cores\u001b[39;00m\n\u001b[1;32m    195\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mtotal tracks downloaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(track_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124m    length of chunked_tracks: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunked_tracks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124m    and inner dims: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[\u001b[38;5;28mlen\u001b[39m(x)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mx\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mchunked_tracks]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    200\u001b[0m )\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m track_chunk \u001b[38;5;129;01min\u001b[39;00m chunked_tracks:\n",
      "Cell \u001b[0;32mIn[9], line 189\u001b[0m, in \u001b[0;36mcall_spotify_api_artist.<locals>.divide_chunks\u001b[0;34m(l, n)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdivide_chunks\u001b[39m(l, n):\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;66;03m# looping till length l\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m l[i:i \u001b[38;5;241m+\u001b[39m n]\n",
      "\u001b[0;31mValueError\u001b[0m: range() arg 3 must not be zero"
     ]
    }
   ],
   "source": [
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "# from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "#                         OutputPath, component, Metrics)\n",
    "\n",
    "\n",
    "def call_spotify_api_artist(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    unique_table: str,\n",
    "    batch_size: int,\n",
    "    batches_to_store: int,\n",
    "    client_id: str,\n",
    "    client_secret: str,\n",
    "    sleep_param: float,\n",
    "    target_table: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('done_message', str),\n",
    "]):\n",
    "    print(f'pip install complete')\n",
    "    \n",
    "    import os\n",
    "    \n",
    "    import spotipy\n",
    "    from spotipy.oauth2 import SpotifyClientCredentials\n",
    "    \n",
    "    import re\n",
    "    import warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    \n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import time\n",
    "    import numpy as np\n",
    "    \n",
    "    from google.cloud import storage\n",
    "    import gcsfs\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    from requests.exceptions import ReadTimeout, HTTPError, ConnectionError, RequestException\n",
    "    from absl import logging\n",
    "\n",
    "    import pandas_gbq\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "    from google.cloud.exceptions import NotFound\n",
    "    \n",
    "    from multiprocessing import Process\n",
    "    import multiprocessing\n",
    "    import time\n",
    "\n",
    "    logging.set_verbosity(logging.INFO)\n",
    "\n",
    "    storage_client = storage.Client(\n",
    "        project=project\n",
    "    )\n",
    "    \n",
    "    logging.info(f'spotipy auth complete')\n",
    "    \n",
    "    def spot_artist_features(uri, client_id, client_secret):\n",
    "\n",
    "        # Authenticate\n",
    "        client_credentials_manager = SpotifyClientCredentials(\n",
    "            client_id=client_id, \n",
    "            client_secret=client_secret\n",
    "        )\n",
    "        sp = spotipy.Spotify(\n",
    "            client_credentials_manager = client_credentials_manager, \n",
    "            requests_timeout=6, \n",
    "            retries=0\n",
    "        )\n",
    "        \n",
    "        # print(\"sp:\", sp)\n",
    "        ############################################################################\n",
    "        # Create Track Audio Features DF\n",
    "        ############################################################################ \n",
    "\n",
    "        # print(\"@@@ uri:\", uri)\n",
    "        artists = sp.artists(uri)\n",
    "        # print(\"@artists:\", artists)\n",
    "        features = pd.json_normalize(artists['artists'])\n",
    "        smaller_features = features[['genres', 'popularity', 'name', 'followers.total']]\n",
    "        smaller_features.columns = ['genres_list',  'artist_pop', 'name',  'followers']\n",
    "        smaller_features['artist_uri'] = uri\n",
    "        smaller_features['genres'] = smaller_features['genres_list'].map(lambda x: f\"{x}\")\n",
    "        # time.sleep(1)\n",
    "        return smaller_features[['genres', 'artist_pop', 'artist_uri', 'followers']]\n",
    "        \n",
    "\n",
    "    bq_client = bigquery.Client(\n",
    "      project=project\n",
    "    )\n",
    "\n",
    "    #check if target table exists and if so return a list to not duplicate records\n",
    "    try:\n",
    "        bq_client.get_table(target_table)  # Make an API request.\n",
    "        logging.info(\"Table {} already exists.\".format(target_table))\n",
    "        target_table_incomplete_query = f\"select distinct artist_uri from `{target_table}`\"\n",
    "        loaded_tracks_df = bq_client.query(target_table_incomplete_query).result().to_dataframe()\n",
    "        loaded_tracks = loaded_tracks_df.artist_uri.to_list()\n",
    "        \n",
    "    except NotFound:\n",
    "        logging.info(\"Table {} is not found.\".format(target_table))\n",
    "        \n",
    "        \n",
    "    query = f\"select distinct artist_uri from `{unique_table}`\"\n",
    "    \n",
    "\n",
    "    schema = [\n",
    "        {'name': 'artist_pop', 'type': 'INTEGER'},\n",
    "        {'name':'genres', 'type': 'STRING'},\n",
    "        {'name':'followers', 'type': 'INTEGER'},\n",
    "        {'name':'artist_uri', 'type': 'STRING'}\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    tracks = bq_client.query(query).result().to_dataframe()\n",
    "    # print(tracks)\n",
    "    track_list = tracks.artist_uri.to_list()\n",
    "    logging.info(f'finished downloading tracks')\n",
    "    \n",
    "    try:\n",
    "        track_list = list(set(track_list) - set(loaded_tracks)) #sets the new track list to remove already loaded data in BQ\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    from tqdm import tqdm\n",
    "    def process_track_list(track_list):\n",
    "        uri_list_length = len(track_list)-1 #starting count at zero\n",
    "        inner_batch_count = 0 #avoiding calling the api on 0th iteration\n",
    "        uri_batch = []\n",
    "        for i, uri in enumerate(tqdm(track_list)):\n",
    "            uri_batch.append(uri)\n",
    "            # print(\"i:\", i, \"uri:\", uri)\n",
    "            if (len(uri_batch) == batch_size or uri_list_length == i) and i > 0: #grab a batch of 50 songs\n",
    "                ### Try catch block for function\n",
    "                try:\n",
    "                    audio_featureDF = spot_artist_features(uri_batch, client_id, client_secret)\n",
    "                    # print(\"audio_featureDF\", audio_featureDF)\n",
    "                    uri_batch = []\n",
    "                \n",
    "                except ReadTimeout:\n",
    "                    logging.info(\"'Spotify timed out... trying again...'\")\n",
    "                    audio_featureDF = spot_artist_features(uri_batch, client_id, client_secret)\n",
    "                    uri_batch = []\n",
    "                    \n",
    "                \n",
    "                except HTTPError as err: #JW ADDED\n",
    "                    logging.info(f\"HTTP Error: {err}\")\n",
    "                \n",
    "                except spotipy.exceptions.SpotifyException as spotify_error: #jw_added\n",
    "                    logging.info(f\"Spotify error: {spotify_error}\")\n",
    "                    \n",
    "                # time.sleep(sleep_param)\n",
    "                \n",
    "                # Accumulate batches on the machine before writing to BQ\n",
    "                # if inner_batch_count <= batches_to_store or uri_list_length == i:\n",
    "                if inner_batch_count == 0:\n",
    "                    appended_data = audio_featureDF\n",
    "                    # logging.info(f\"creating new appended data at IBC: {inner_batch_count} \\n i: {i}\")\n",
    "                    inner_batch_count += 1\n",
    "                \n",
    "                elif uri_list_length == i or inner_batch_count == batches_to_store: #send the batches to bq\n",
    "                    appended_data = pd.concat([audio_featureDF, appended_data])\n",
    "                    inner_batch_count = 0\n",
    "                    appended_data.to_gbq(\n",
    "                        destination_table=target_table, \n",
    "                        project_id=f'{project}', \n",
    "                        location='US', \n",
    "                        table_schema=schema,\n",
    "                        progress_bar=False, \n",
    "                        reauth=False, \n",
    "                        if_exists='append'\n",
    "                    )\n",
    "                    logging.info(f'{i+1} of {uri_list_length} complete!')\n",
    "                \n",
    "                else:\n",
    "                    appended_data = pd.concat([audio_featureDF, appended_data])\n",
    "                    inner_batch_count += 1\n",
    "\n",
    "        logging.info(f'audio features appended')\n",
    "    \n",
    "    #multiprocessing portion - we will loop based on the modulus of the track_uri list\n",
    "    #chunk the list \n",
    "    \n",
    "    # Yield successive n-sized\n",
    "    # chunks from l.\n",
    "    def divide_chunks(l, n):\n",
    "        # looping till length l\n",
    "        for i in range(0, len(l), n):\n",
    "            yield l[i:i + n]\n",
    "            \n",
    "    n_cores = multiprocessing.cpu_count() \n",
    "    chunked_tracks = list(divide_chunks(track_list, int(len(track_list)/n_cores))) # produces a list of lists chunked evenly by groups of n_cores\n",
    "    \n",
    "    logging.info(\n",
    "        f\"\"\"total tracks downloaded: {len(track_list)}\\n\n",
    "        length of chunked_tracks: {len(chunked_tracks)}\\n \n",
    "        and inner dims: {[len(x) for x in chunked_tracks]}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    for track_chunk in chunked_tracks:\n",
    "        process_track_list(track_chunk)\n",
    "\n",
    "        \n",
    "    # process_track_list(track_list)\n",
    "    logging.info(f'artist features appended')\n",
    "    \n",
    "    return (\n",
    "          f'DONE',\n",
    "      )\n",
    "\n",
    "# client_id=\"f4c3b0103b144195888f586f767cb748\"\n",
    "# client_secret=\"d36e9279fa8e4f12b98ef72a2e1f6a72\"\n",
    "call_spotify_api_artist(\n",
    "    project=\"myproject32549\",\n",
    "    location=\"myproject32549\",\n",
    "    unique_table=\"myproject32549.spotify_e2e_test2.tracks_unique\",\n",
    "    batch_size=50,\n",
    "    batches_to_store=50,\n",
    "    client_id=\"f4c3b0103b144195888f586f767cb748\",\n",
    "    client_secret=\"d36e9279fa8e4f12b98ef72a2e1f6a72\",\n",
    "    sleep_param=0.5,\n",
    "    target_table=\"myproject32549.spotify_e2e_test2.artist_features\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip install complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Table myproject32549.spotify_e2e_test2.audio_features already exists.\n",
      "INFO:absl:finished downloading tracks\n",
      "INFO:absl:\n",
      "        total tracks downloaded: 2002207\n",
      "\n",
      "        length of chunked_tracks: 1\n",
      " \n",
      "        and inner dims: [2002207]\n",
      "        \n",
      "  0%|          | 0/2002207 [00:00<?, ?it/s]ERROR:spotipy.client:Max Retries reached\n",
      "INFO:absl:Spotify error: http status: 429, code:-1 - /v1/audio-features/?ids=4wsNcZOMiU2ThJ0AhM0282,6h0MJAtzHMetVD8718zVgP,4RrbfMhc1AKNhylRC1mp1U,5KNBHbtDfOW48zjAnfY5iU,5F3i5W6FTuWtBFGmvckaO1,5qvThjvpC7q5paS9DCzDA1,6qJ6u80mylRXG7wcFTdr2w,3jLHXebqQ802XeP9fnnMWz,2cViIXIe8Pbd1sOJExMJlK,7abfUxNpRHuby0N2T4JCv1,1ovGM7ayJfoYYXkkF7Wzkb,1nEPEGmRi3o8ybsCepKaEp,3EW3BFkqqy5XDs9sUcBsJh,5ryqIuIKgjfgKXHSxy7gzn,7xcWmcZe7APzr8pNFP2nn3,1c5xEBeV4BbzDqmBmB5PSH,4gtER6hj0kb1amaXAHYlkk,1HWVbYxnJjMTBKEZNXRarz,001ayA5alFiUVW2fw5jpyW,2MLbGztItx5Lxytg40Bt8v,3prjpJbL2pAhaBfVrRAL2m,152601s4UKWKo784aQDGKm,5cRTQzUWa4jpiL1xCWOJTr,754DWmELcRbJ8nSxpidPt4,2QixklKF002TyGCQfzPWLZ,5s43J0J4GCXTvQ1qqwMfBm,2wUYb8RhJAZwRJiaxX3fr6,3xpsG0ZZ9IPvSjBzdkN2CW,4s4B4L94OJcz66RJpuqfjy,4Un6pB4KZKekFh3SVIa9qB,67ZYGALvwzHPB8tTSfXLJa,2Bkg6Vz0V1aC0WzEvSyOaB,6jYfZbK2GmfwwLLOYVudrI,6cM1PTBXjlkaG5jFuBiJOg,3RqsErTqRvEC5gSgB4pmgW,6V0aCjUMxvBPGJkesZjxvj,67Fvlz9VuJJ0p22gZSYOiE,66dpwcGJOKz3w0Ak9z6DDe,3J8Z7R1EXJvj5vGVzhOMpv,0q3DSQE5srkobKA0qd0OIv,4FXTOvMOeRlZE5RkvXA0OF,3fLLoWz8hjRpbWttMr5iFV,08ER5gGec5TzkQ3kT77Fbk,5ncxbtFf5YTQ7b7B1K2THv,6vngujtnA3ptFQjm1AVnpg,1yyE8g4hHUgsHLe8PFYuzv,4ROhuTgPkkvNcTbgRaxoHz,1xodvM40EfWvQk1ndk5YFm,3UetJexcdZHKYGxNHnaxUw,5hkgrWxkobGtg30I7DsfVu:\n",
      " Max Retries, reason: too many 429 error responses\n",
      "  0%|          | 49/2002207 [00:02<32:03:38, 17.35it/s]\n",
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/tmp/ipykernel_3006/603815233.py\", line 172, in process_track_list\n",
      "    appended_data = audio_featureDF\n",
      "UnboundLocalError: local variable 'audio_featureDF' referenced before assignment\n",
      "INFO:absl:audio features appended\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'DONE'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "\n",
    "def call_spotify_api_audio(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    client_id: str,\n",
    "    batch_size: int,\n",
    "    batches_to_store: int,\n",
    "    target_table: str,\n",
    "    client_secret: str,\n",
    "    unique_table: str,\n",
    "    sleep_param: float,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('done_message', str),\n",
    "]):\n",
    "    print(f'pip install complete')\n",
    "    import os\n",
    "    \n",
    "    import spotipy\n",
    "    from spotipy.oauth2 import SpotifyClientCredentials\n",
    "    \n",
    "    import re\n",
    "    import warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    \n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import time\n",
    "    import numpy as np\n",
    "    \n",
    "    from requests.exceptions import ReadTimeout, HTTPError, ConnectionError, RequestException\n",
    "    from absl import logging\n",
    "    \n",
    "    from google.cloud import storage\n",
    "    import gcsfs\n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    import pandas_gbq\n",
    "    from multiprocessing import Process\n",
    "    from tqdm import tqdm\n",
    "    from tqdm.contrib.logging import logging_redirect_tqdm\n",
    "    \n",
    "    from google.cloud.exceptions import NotFound\n",
    "\n",
    "    import multiprocessing\n",
    "\n",
    "    # print(f'package import complete')\n",
    "\n",
    "    logging.set_verbosity(logging.INFO)\n",
    "\n",
    "    \n",
    "    bq_client = bigquery.Client(\n",
    "      project=project, location=location\n",
    "    )\n",
    "    \n",
    "    def spot_audio_features(uri, client_id, client_secret):\n",
    "\n",
    "        # Authenticate\n",
    "        client_credentials_manager = SpotifyClientCredentials(\n",
    "            client_id=client_id, \n",
    "            client_secret=client_secret\n",
    "        )\n",
    "        sp = spotipy.Spotify(\n",
    "            client_credentials_manager = client_credentials_manager, \n",
    "            requests_timeout=10, \n",
    "            retries=10\n",
    "        )\n",
    "        ############################################################################\n",
    "        # Create Track Audio Features DF\n",
    "        ############################################################################\n",
    "        \n",
    "        uri_stripped = [u.replace('spotify:track:', '') for u in uri] #fix the quotes \n",
    "        #getting track popularity\n",
    "        tracks = sp.tracks(uri_stripped)\n",
    "        #Audio features\n",
    "        time.sleep(sleep_param)\n",
    "    \n",
    "        a_feats = sp.audio_features(uri)\n",
    "        features = pd.json_normalize(a_feats)#.to_dict('list')\n",
    "        \n",
    "        features['track_pop'] = pd.json_normalize(tracks['tracks'])['popularity']\n",
    "        \n",
    "        features['track_uri'] = uri\n",
    "        return features\n",
    "\n",
    "    bq_client = bigquery.Client(\n",
    "        project=project, \n",
    "        location='US'\n",
    "    )\n",
    "    \n",
    "    #check if target table exists and if so return a list to not duplicate records\n",
    "    try:\n",
    "        bq_client.get_table(target_table)  # Make an API request.\n",
    "        logging.info(\"Table {} already exists.\".format(target_table))\n",
    "        target_table_incomplete_query = f\"select distinct track_uri from `{target_table}`\"\n",
    "        loaded_tracks_df = bq_client.query(target_table_incomplete_query).result().to_dataframe()\n",
    "        loaded_tracks = loaded_tracks_df.track_uri.to_list()\n",
    "        \n",
    "    except NotFound:\n",
    "        logging.info(\"Table {} is not found.\".format(target_table))\n",
    "    \n",
    "    query = f\"select distinct track_uri from `{unique_table}`\" \n",
    "\n",
    "    #refactor\n",
    "    schema = [{'name':'danceability', 'type': 'FLOAT', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'energy', 'type': 'FLOAT', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'key', 'type': 'FLOAT', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'loudness', 'type': 'FLOAT', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'mode', 'type': 'INTEGER', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'speechiness', 'type': 'FLOAT', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'acousticness', 'type': 'FLOAT', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'instrumentalness', 'type': 'FLOAT', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'liveness', 'type': 'FLOAT', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'valence', 'type': 'FLOAT', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'tempo', 'type': 'FLOAT', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'type', 'type': 'STRING', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'id', 'type': 'STRING', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'uri', 'type': 'STRING', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'track_href', 'type': 'STRING', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'analysis_url', 'type': 'STRING', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'duration_ms', 'type': 'INTEGER', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'time_signature', 'type': 'INTEGER', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'track_pop', 'type': 'INTEGER', \"mode\": \"NULLABLE\"},\n",
    "        {'name':'track_uri', 'type': 'STRING', \"mode\": \"REQUIRED\"},\n",
    "    ]\n",
    "    \n",
    "    tracks = bq_client.query(query).result().to_dataframe()\n",
    "    track_list = tracks.track_uri.to_list()\n",
    "    logging.info(f'finished downloading tracks')\n",
    "    \n",
    "    \n",
    "    ### This section is used when there are tracks already loaded into BQ and you want to resume loading the data\n",
    "    try:\n",
    "        track_list = list(set(track_list) - set(loaded_tracks)) #sets the new track list to remove already loaded data in BQ\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    from tqdm import tqdm\n",
    "    def process_track_list(track_list):\n",
    "        \n",
    "        uri_list_length = len(track_list)-1 #starting count at zero\n",
    "        inner_batch_count = 0 #avoiding calling the api on 0th iteration\n",
    "        uri_batch = []\n",
    "        \n",
    "        for i, uri in enumerate(tqdm(track_list)):\n",
    "            uri_batch.append(uri)\n",
    "            if (len(uri_batch) == batch_size or uri_list_length == i) and i > 0: #grab a batch of 50 songs\n",
    "                    # logging.info(f\"appending final record for nth song at: {inner_batch_count} \\n i: {i} \\n uri_batch length: {len(uri_batch)}\")\n",
    "                    ### Try catch block for function\n",
    "                try:\n",
    "                    audio_featureDF = spot_audio_features(uri_batch, client_id, client_secret)\n",
    "                    time.sleep(sleep_param)\n",
    "                    uri_batch = []\n",
    "                except ReadTimeout:\n",
    "                    logging.info(\"'Spotify timed out... trying again...'\")\n",
    "                    audio_featureDF = spot_audio_features(uri_batch, client_id, client_secret)\n",
    "                    \n",
    "                    uri_batch = []\n",
    "                    time.sleep(sleep_param)\n",
    "                \n",
    "                except HTTPError as err: #JW ADDED\n",
    "                    logging.info(f\"HTTP Error: {err}\")\n",
    "                \n",
    "                except spotipy.exceptions.SpotifyException as spotify_error: #jw_added\n",
    "                    logging.info(f\"Spotify error: {spotify_error}\")\n",
    "                    \n",
    "                # Accumulate batches on the machine before writing to BQ\n",
    "                # if inner_batch_count <= batches_to_store or uri_list_length == i:\n",
    "                if inner_batch_count == 0:\n",
    "                    appended_data = audio_featureDF\n",
    "                    # logging.info(f\"creating new appended data at IBC: {inner_batch_count} \\n i: {i}\")\n",
    "                    inner_batch_count += 1\n",
    "                elif uri_list_length == i or inner_batch_count == batches_to_store: #send the batches to bq\n",
    "                    appended_data = pd.concat([audio_featureDF, appended_data])\n",
    "                    inner_batch_count = 0\n",
    "                    appended_data.to_gbq(\n",
    "                        destination_table=target_table, \n",
    "                        project_id=f'{project}', \n",
    "                        location='US', \n",
    "                        table_schema=schema,\n",
    "                        progress_bar=False, \n",
    "                        reauth=False, \n",
    "                        if_exists='append'\n",
    "                    )\n",
    "                    logging.info(f'{i+1} of {uri_list_length} complete!')\n",
    "                else:\n",
    "                    appended_data = pd.concat([audio_featureDF, appended_data])\n",
    "                    inner_batch_count += 1\n",
    "\n",
    "        logging.info(f'audio features appended')\n",
    "    \n",
    "    #multiprocessing portion - we will loop based on the modulus of the track_uri list\n",
    "    #chunk the list \n",
    "    \n",
    "    # Yield successive n-sized\n",
    "    # chunks from l.\n",
    "    def divide_chunks(l, n):\n",
    "        # looping till length l\n",
    "        for i in range(0, len(l), n):\n",
    "            yield l[i:i + n]\n",
    "            \n",
    "    n_cores = multiprocessing.cpu_count() \n",
    "    chunked_tracks = list(divide_chunks(track_list, int(len(track_list)/n_cores))) #produces a list of lists chunked evenly by groups of n_cores\n",
    "    \n",
    "    logging.info(\n",
    "        f\"\"\"\n",
    "        total tracks downloaded: {len(track_list)}\\n\n",
    "        length of chunked_tracks: {len(chunked_tracks)}\\n \n",
    "        and inner dims: {[len(x) for x in chunked_tracks]}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    procs = []\n",
    "    \n",
    "    def create_job(target, *args):\n",
    "        p = multiprocessing.Process(target=target, args=args)\n",
    "        p.start()\n",
    "        return p\n",
    "\n",
    "    # starting process with arguments\n",
    "    for track_chunk in chunked_tracks:\n",
    "        proc = create_job(process_track_list, track_chunk)\n",
    "        time.sleep(np.pi)\n",
    "        procs.append(proc)\n",
    "\n",
    "    # complete the processes\n",
    "    for proc in procs:\n",
    "        proc.join()\n",
    "        \n",
    "    # process_track_list(track_list) #single thread\n",
    "    logging.info(f'audio features appended')\n",
    "    \n",
    "    return f'DONE'\n",
    "\n",
    "\n",
    "call_spotify_api_audio(\n",
    "    project=\"myproject32549\",\n",
    "    location=\"myproject32549\",\n",
    "    client_id=\"f4c3b0103b144195888f586f767cb748\",\n",
    "    batch_size=50,\n",
    "    batches_to_store=50,\n",
    "    target_table=\"myproject32549.spotify_e2e_test2.audio_features\",\n",
    "    client_secret=\"d36e9279fa8e4f12b98ef72a2e1f6a72\",\n",
    "    unique_table=\"myproject32549.spotify_e2e_test2.tracks_unique\",\n",
    "    sleep_param=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/0001wHqxbF2YYRQxGdbyER'}, 'followers': {'href': None, 'total': 9898}, 'genres': ['progressive psytrance'], 'href': 'https://api.spotify.com/v1/artists/0001wHqxbF2YYRQxGdbyER', 'id': '0001wHqxbF2YYRQxGdbyER', 'images': [{'height': 640, 'url': 'https://i.scdn.co/image/ab67616d0000b273bdb6955d7b55866131d86f98', 'width': 640}, {'height': 300, 'url': 'https://i.scdn.co/image/ab67616d00001e02bdb6955d7b55866131d86f98', 'width': 300}, {'height': 64, 'url': 'https://i.scdn.co/image/ab67616d00004851bdb6955d7b55866131d86f98', 'width': 64}], 'name': 'Motion Drive', 'popularity': 11, 'type': 'artist', 'uri': 'spotify:artist:0001wHqxbF2YYRQxGdbyER'}, {'external_urls': {'spotify': 'https://open.spotify.com/artist/000UxvYLQuybj6iVRRCAw1'}, 'followers': {'href': None, 'total': 116}, 'genres': [], 'href': 'https://api.spotify.com/v1/artists/000UxvYLQuybj6iVRRCAw1', 'id': '000UxvYLQuybj6iVRRCAw1', 'images': [{'height': 640, 'url': 'https://i.scdn.co/image/ab67616d0000b273e5dd508152ff1152bcd1f6af', 'width': 640}, {'height': 300, 'url': 'https://i.scdn.co/image/ab67616d00001e02e5dd508152ff1152bcd1f6af', 'width': 300}, {'height': 64, 'url': 'https://i.scdn.co/image/ab67616d00004851e5dd508152ff1152bcd1f6af', 'width': 64}], 'name': 'Primera Etica', 'popularity': 0, 'type': 'artist', 'uri': 'spotify:artist:000UxvYLQuybj6iVRRCAw1'}]}\n"
     ]
    }
   ],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "client_id=\"f4c3b0103b144195888f586f767cb748\"\n",
    "client_secret=\"d36e9279fa8e4f12b98ef72a2e1f6a72\"\n",
    "\n",
    "# Authenticate\n",
    "client_credentials_manager = SpotifyClientCredentials(\n",
    "    client_id=client_id, \n",
    "    client_secret=client_secret,\n",
    "    # requests_session=False,\n",
    ")\n",
    "sp = spotipy.Spotify(\n",
    "    client_credentials_manager = client_credentials_manager, \n",
    "    requests_timeout=50, \n",
    "    retries=0 \n",
    "    )\n",
    "\n",
    "# 0001wHqxbF2YYRQxGdbyER,000UxvYLQuybj6iVRRCAw1\n",
    "\n",
    "uri=['spotify:artist:0001wHqxbF2YYRQxGdbyER', 'spotify:artist:000UxvYLQuybj6iVRRCAw1']\n",
    "try:\n",
    "  artists = sp.artists(uri)\n",
    "  print(artists)\n",
    "except Exception as ex:\n",
    "  # print(artists.get(\"Retry-After\"))\n",
    "  print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### submit pipeline job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'job' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mjob\u001b[49m\u001b[38;5;241m.\u001b[39msubmit()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'job' is not defined"
     ]
    }
   ],
   "source": [
    "job.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Vertex Pipeline UI (console) \n",
    "\n",
    "* It may take a couple of runs with different application credientials for the audio tracks\n",
    "* This module can resume from where data was already loaded to BQ\n",
    "\n",
    "<!-- ![](img/feature-enrich-pipeline.png) -->\n",
    "\n",
    "<img\n",
    "  src=\"img/feature-enrich-pipeline.png\"\n",
    "  alt=\"Alt text\"\n",
    "  title=\"Feature Enrichment Pipeline\"\n",
    "  style=\"display: inline-block; margin: 0 auto; max-width: 900px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GitIgnore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .gitignore\n"
     ]
    }
   ],
   "source": [
    "%%writefile .gitignore\n",
    "*.cpython-310.pyc\n",
    "*-checkpoint.*\n",
    "*-checkpoint.md\n",
    "*-checkpoint.py\n",
    "*.ipynb_checkpoints\n",
    "# .gcloudignore\n",
    "# .git\n",
    "# .github\n",
    ".ipynb_checkpoints/*\n",
    "*__pycache__\n",
    "# *cpython-37.pyc\n",
    "# .gitignore\n",
    "# .DS_Store\n",
    "spotipy_secret_creds.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "Now that all data is loaded in BigQuery, the [01-bq-data-prep.ipynb](01-bq-data-prep.ipynb) notebook will finish feature prep"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Extract-spotify-features-pipeline-parallel-for.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-gpu.2-8:m119"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
